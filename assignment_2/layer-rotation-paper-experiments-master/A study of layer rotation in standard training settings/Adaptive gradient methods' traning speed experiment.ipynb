{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment where layca is used on top of SGD, and copies the layer rotation rates of adaptive gradient method training of the same task. Goal is to show that adaptive gradient methods' training speed improvements are mostly due to changes to the layer rotations.\n",
    "\n",
    "results are visualized in 'Analysis of Adaptive Gradient Methods' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "\n",
    "import math as m\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from experiment_utils import history_todict, get_val_split\n",
    "from layer_rotation_utils import LayerRotationCurves,StepwiseRotation, StepwiseLearningRateScheduler\n",
    "from layca_optimizers import SGD\n",
    "\n",
    "from import_task import import_task\n",
    "from get_training_utils import get_training_schedule, get_stopping_criteria, get_optimizer, get_learning_rate_multipliers\n",
    "from get_training_utils import get_optimized_training_schedule\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities for storing the results in pickle files\n",
    "result_file = 'results_explanatory_adaptive.p'\n",
    "def load_results():\n",
    "    if not os.path.isfile(result_file):\n",
    "        return {}\n",
    "    else:\n",
    "        with open(result_file,'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def dump_results(results):\n",
    "    with open(result_file,'wb') as f:\n",
    "        pickle.dump(dict(results),f)\n",
    "\n",
    "def update_results(path, new_data):\n",
    "    results = load_results()\n",
    "    position = results\n",
    "    for p in path:\n",
    "        position = position[p]\n",
    "    # new_data is a dictionary with the new (key,value) pairs\n",
    "    position.update(new_data)\n",
    "    dump_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if results should be saved in the file or not\n",
    "save_results = True\n",
    "if not save_results:\n",
    "    results = {}\n",
    "# file for monitoring the experiment's progress\n",
    "monitor_file = 'monitor_explanatory_adaptive.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = ['C10-CNN2']#['C10-CNN1','C100-resnet','tiny-CNN','C10-CNN2','C100-WRN']\n",
    "\n",
    "for task in tasks:\n",
    "    x_train, y_train, x_test, y_test, get_model = import_task(task)\n",
    "    \n",
    "    # validation set is actually not needed for this experiment... (I forgot to remove it but it doesn't matter)\n",
    "    [x_train, y_train], [x_val, y_val] = get_val_split(x_train,y_train, 0.1)\n",
    "    \n",
    "    # creates empty dictionary if first time the task is seen\n",
    "    if save_results:\n",
    "        results = load_results()\n",
    "        if task not in results.keys():\n",
    "            update_results([],{task:{}})\n",
    "    elif task not in results.keys():\n",
    "        results.update({task:{}})\n",
    "    \n",
    "    if task == 'C10-CNN1':\n",
    "        optimizers = ['SGD','RMSprop','SGD_layca']\n",
    "    elif task == 'C100-resnet':\n",
    "        optimizers = ['SGD','Adam','SGD_AMom_layca']\n",
    "    elif task == 'tiny-CNN':\n",
    "        optimizers = ['SGD','Adagrad','SGD_layca']\n",
    "    elif task == 'C10-CNN2':\n",
    "        optimizers = ['SGD_weight_decay','RMSprop_weight_decay','SGD_layca']\n",
    "    elif task == 'C100-WRN':\n",
    "        optimizers = ['SGD_weight_decay','Adam_weight_decay','SGD_AMom_layca']\n",
    "    \n",
    "    for optimizer in optimizers:\n",
    "        start = time.time()\n",
    "        model = get_model(weight_decay = 0.) if 'weight_decay' not in optimizer else get_model()\n",
    "\n",
    "        batch_size = 128\n",
    "        if 'layca' not in optimizer:\n",
    "            epochs, lr, lr_scheduler = get_optimized_training_schedule(task,optimizer)\n",
    "        else: # when using layca, we don't want to get best schedule, but to copy the schedule of the adaptive method\n",
    "            epochs, lr, lr_scheduler = get_optimized_training_schedule(task,optimizers[1])\n",
    "        verbose = 0\n",
    "\n",
    "        # frequency at which cosine distance from initialization is computed\n",
    "        batch_frequency = int((x_train.shape[0]/batch_size))+5 # higher value than # of batches per epoch means once per epoch\n",
    "        ladc = LayerRotationCurves(batch_frequency = batch_frequency)\n",
    "        callbacks = [lr_scheduler, ladc]\n",
    "\n",
    "        stepwise_recordings = StepwiseRotation()\n",
    "        if not 'SGD' in optimizer:\n",
    "            callbacks += [stepwise_recordings]\n",
    "        if 'layca' in optimizer:\n",
    "            if save_results:\n",
    "                results = load_results()\n",
    "            stepwise_schedule = StepwiseLearningRateScheduler(schedule = results[task][optimizers[1]]['stepwise_recordings'])\n",
    "            callbacks+= [stepwise_schedule]\n",
    "\n",
    "        multipliers = get_learning_rate_multipliers(model,alpha = 0.)\n",
    "        # C100-WRN + SGD is the only case where nesterov momentum is used (cfr. original implementation)\n",
    "        if task == 'C100-WRN' and optimizer in ['SGD','SGD_weight_decay']: \n",
    "            opt = SGD(lr=lr, momentum=0.9, nesterov=True,multipliers = multipliers)\n",
    "        else:\n",
    "            opt = get_optimizer(optimizer, lr,multipliers)\n",
    "        metrics = ['accuracy', 'top_k_categorical_accuracy'] if 'tiny' in task else ['accuracy']\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer= opt,\n",
    "                      metrics=metrics)\n",
    "\n",
    "        # cifar100 resnet and tinyImagenet need early stopping\n",
    "        if task=='C100-resnet' or 'tiny' in task:\n",
    "            weights_file = 'saved_weights/best_weights_'+str(np.random.randint(1e6))+'.h5'\n",
    "            callbacks += [ModelCheckpoint(weights_file, monitor='val_acc', save_best_only=True, save_weights_only = True)]\n",
    "\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            if task in ['C10-CNN2','C100-WRN']:\n",
    "                # data augmentation\n",
    "                datagen = ImageDataGenerator(width_shift_range=0.125,\n",
    "                         height_shift_range=0.125,\n",
    "                         fill_mode='reflect',\n",
    "                         horizontal_flip=True)\n",
    "\n",
    "                warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "                history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                                              steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                                              epochs = epochs,\n",
    "                                              verbose = verbose,\n",
    "                                              validation_data = (x_val, y_val),\n",
    "                                              callbacks = callbacks)\n",
    "            else:\n",
    "                warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "                history = model.fit(x_train,y_train,\n",
    "                                    epochs = epochs,\n",
    "                                    batch_size = batch_size,\n",
    "                                    verbose = verbose,\n",
    "                                    validation_data = (x_val, y_val),\n",
    "                                    callbacks = callbacks)\n",
    "\n",
    "        # application of early stopping\n",
    "        if task=='C100-resnet' or 'tiny' in task:\n",
    "            model.load_weights(weights_file)\n",
    "\n",
    "        test_performance = model.evaluate(x_test,y_test, verbose = verbose)\n",
    "\n",
    "        if save_results:\n",
    "            update_results([task],{optimizer:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                              'stepwise_recordings': stepwise_recordings.memory,\n",
    "                                              'test_performance':test_performance}})\n",
    "        else:\n",
    "            results[task].update({optimizer:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                             'stepwise_recordings': stepwise_recordings.memory,\n",
    "                                             'test_performance':test_performance}})\n",
    "\n",
    "        with open(monitor_file,'a') as file:\n",
    "            file.write(task + ', '+optimizer+': done in '+str(time.time()-start)+' seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
