{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeHe2I9Ix2iM"
      },
      "source": [
        "# SIT319/SIT744 Practical 2: Introduction to TensorBoard and automatic differentiation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSJnyS9Fy51S"
      },
      "source": [
        "‚Ñπ We suggest that you run this notebook using Google Colab.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XQAobcnsUU"
      },
      "source": [
        "## Task 1: Use TensorBoard with PyTorch\n",
        "\n",
        "TensorBoard is a visualization tool that helps monitor PyTorch training, debug models, and compare experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Enable TensorBoard in Colab\n",
        "\n",
        "Colab already includes TensorBoard, but you can ensure it's installed:"
      ],
      "metadata": {
        "id": "GSdKFfHCMfsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "ceNokz49MpFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running this notebook locally, install TensorBoard in your terminal:\n",
        "\n",
        "```shell\n",
        "$ pip install tensorboard\n",
        "```"
      ],
      "metadata": {
        "id": "iS7O3WBkNJDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Create a TensorBoard logger\n",
        "\n",
        "TensorBoard looks for a log folder called logdir, which contains summary data to be visualised. In PyTorch, we define logdir when initializing SummaryWriter"
      ],
      "metadata": {
        "id": "bGUBMW5UOoOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer = SummaryWriter(log_dir=f\"runs/experiment_{int(time.time())}\")"
      ],
      "metadata": {
        "id": "ZVSSVwRdNso7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SummaryWriter` logs data to `runs/experiment_xxx`, which TensorBoard reads.\n",
        "\n",
        "> üìù Check what new folders have been created by the above code. Do you see any files inside the folders?"
      ],
      "metadata": {
        "id": "aeUT1XzWO-QL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjH28jbA3Iw1"
      },
      "source": [
        "Here is how TensorBoard expects logdir to be organised.\n",
        "\n",
        "\n",
        "#### runs\n",
        "\n",
        "A run refers to a separate execution of the model, typically stored as a subfolder under logdir. Multiple runs allow comparison of different experiments."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer1 = SummaryWriter(log_dir=\"runs/experiment_1\")\n",
        "writer2 = SummaryWriter(log_dir=\"runs/experiment_2\")"
      ],
      "metadata": {
        "id": "2rk78ytUQvpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### event files\n",
        "\n",
        "PyTorch's SummaryWriter creates event files in logdir.\n",
        "These files have names like events.out.tfevents.<timestamp>.\n",
        "These are automatically recognized by TensorBoard.\n",
        "\n",
        "Each file contains records called *summaries*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tQJpOFLlQXwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tags\n",
        "\n",
        "You add tags to a summary by passing a `tag` argument in logging calls (See examples below). In TensorBoard, these tags allow you to filter and categorise data to be visualised."
      ],
      "metadata": {
        "id": "b-YjG9nCYuiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging loss to an event file\n",
        "for epoch in range(10):\n",
        "    loss = 0.05 * epoch  # Dummy loss for illustration\n",
        "    writer.add_scalar(\"Loss/train\", loss, epoch)  # 'Loss/train' is the tag\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "jgrPKqxEYnUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualise the dummy loss written into the first SummaryWriter:"
      ],
      "metadata": {
        "id": "JkeuoDM2ZaWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fs0TZBaAZsKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional exercise:  Logging Dummy Metrics\n",
        "\n",
        "1. Setup TensorBoard:\n",
        " - Import necessary modules and initialize a `SummaryWriter`.\n",
        " - Create a log folder (e.g. \"runs/experiment_xxx\") where event files will be saved.\n",
        "\n",
        "2. Log a Dummy Loss:\n",
        "\n",
        " - Simulate training by iterating over several epochs (e.g. 20).\n",
        " - For each epoch, compute a dummy loss (for example, using a simple mathematical function like `loss = 1/(epoch+1)` or any function you choose) and log it using `writer.add_scalar(\"Loss/train\", loss, epoch)`.\n",
        "\n",
        "3. Visualize in TensorBoard:\n",
        "\n",
        " - Run TensorBoard (e.g. `%tensorboard --logdir runs` in Colab or locally) and check that your loss curve is visible.\n",
        "\n",
        "Additional Challenge:\n",
        "\n",
        "- Log an additional metric (for instance, ‚ÄúAccuracy/train‚Äù) and compare the two curves in TensorBoard.\n",
        "- Create another run with a different simulated training strategy and compare the two runs side-by-side.\n"
      ],
      "metadata": {
        "id": "18g4KSkWhAEa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMNglTF02_73"
      },
      "source": [
        "## Task 2: Visualise computational graph of a PyTorch model\n",
        "\n",
        "Given a PyTorch model, we can use TensorBoard to visualise its Computational Graph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Define a PyTorch model for y = x^2 + x\n",
        "class FooFunction(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x ** 2 + x  # Function: y = x^2 + x\n",
        "\n",
        "# Initialize model\n",
        "model = FooFunction()\n",
        "\n",
        "# Create input tensor\n",
        "x = torch.tensor([[2.0]])  # Must be a tensor inside a list for proper graph logging\n",
        "\n",
        "\n",
        "# Log computational graph to TensorBoard\n",
        "writer.add_graph(model, input_to_model=[x])\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "ggechJunds_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can refresh TensorBoard and you should be able to see a **GRAPHS** tab. There you can find the computation graph. Click inside the `FooFunction` node to understand how the model is implemented."
      ],
      "metadata": {
        "id": "01k-B7fqhsIv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXl4-wW45uI"
      },
      "source": [
        "You should be able to see a **GRAPHS** tab and there you can find the computational graph.\n",
        "> How many *operations* do you see in the computational graph?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbSV8UhjKjhR"
      },
      "source": [
        "> üìù **Exercise**:\n",
        "> 1. Follow the example above to display the computation graph of the function $z = 3 x^2 + 2xy$.\n",
        "  - Define a simple PyTorch model by subclassing nn.Module.\n",
        "  - Create a sample input tensor (ensure it‚Äôs the right shape, e.g. wrapped in a list if needed).\n",
        "  - Use `writer.add_graph(model, input_to_model=[x])` to log the model‚Äôs computational graph to TensorBoard.\n",
        "> 2. Log the value of the sine function from 0 to 100; Display the values in TensorBoard. (*Hint: use a different tag.*)\n",
        "> 3. Create another run. And plot the cosine function instead. (*Hint: use a different summary writer.*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P82VgscODrJO"
      },
      "source": [
        "## Task 3. PyTorch Automatic Differentiation (Autograd)\n",
        "\n",
        "Computing gradient is a core requirement for training deep learning models. PyTorch's autograd module enables automatic differentiation, making it easy to compute gradients."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input tensor with requires_grad=True to track gradients\n",
        "x = torch.tensor([[2.0]], requires_grad=True)\n",
        "\n",
        "# Forward pass: Compute y = x^2 + x\n",
        "y = model(x)"
      ],
      "metadata": {
        "id": "O41C6ajTl9lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> üìù What is the value of $y$? Use pen and paper to work out  the gradient $‚àáy$?"
      ],
      "metadata": {
        "id": "OCQbtYTNmEfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd‚Äôs internal gradient functions (`grad_fn`) are added in the forward pass."
      ],
      "metadata": {
        "id": "_0VfITxdKU0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the recorded gradient function in the graph\n",
        "print(y.grad_fn)  # Shows the last operation in the graph\n",
        "\n",
        "# Show the chain of operations in the graph\n",
        "print(y.grad_fn.next_functions)\n",
        "\n",
        "# Going one level deeper\n",
        "print(y.grad_fn.next_functions[0][0].next_functions)"
      ],
      "metadata": {
        "id": "W353I6C5KnQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Can you map these `grad_fn` to their respective operation nodes in the computational graph?"
      ],
      "metadata": {
        "id": "x3xxTx5rSjxa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FljRqmmM4Syj"
      },
      "source": [
        "# Compute gradients (dy/dx)\n",
        "y.backward()\n",
        "\n",
        "# Print gradient\n",
        "print(f\"Gradient dy/dx at x={x.item()}: {x.grad.item()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disabling Autograd (No Gradient Tracking)\n",
        "\n",
        "Sometimes, we don‚Äôt need gradients, e.g., during inference.\n",
        "Disable autograd using `.detach()` or `torch.no_grad()`. Stopping unnecessary gradient tracking saves GPU memory."
      ],
      "metadata": {
        "id": "f1MCC2LaoGJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.requires_grad)\n",
        "\n",
        "# Create a tensor without gradient tracking\n",
        "z = y.detach()\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "id": "I8jQKrmRoQHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `torch.no_grad()`"
      ],
      "metadata": {
        "id": "ymLRSgS6oxrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y = x ** 2\n",
        "print(y.requires_grad)  # Output: False"
      ],
      "metadata": {
        "id": "4PEav8Boot_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dLgkI7BA0-4"
      },
      "source": [
        "**Exercise**: Follow the example above to compute the gradient of function $z(x,y) = 3 x^2 + 2xy$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional exercise: Compute Gradients for a Custom Function\n",
        "\n",
        "1. Define a Function with Two Inputs:\n",
        "\n",
        " - For example, define $z(x,y)=3x^2+2xy$.\n",
        " - Create tensors for x and y with requires_grad=True.\n",
        "\n",
        "2. Compute the Function and Backward Pass:\n",
        " - Calculate $z$ using the defined formula.\n",
        " - Call `z.backward()` to compute the gradients.\n",
        " - Print the gradients of `x` and `y`.\n",
        "\n",
        "3. Disable Gradient Tracking:\n",
        "\n",
        " - Use `torch.no_grad()` or the `.detach()` method to perform an operation without tracking gradients.\n",
        " - Verify by checking the `requires_grad` property of the output."
      ],
      "metadata": {
        "id": "UmJez5G2jstw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbd7zbd1NW42"
      },
      "source": [
        "## Additional resources\n",
        "\n",
        "- [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "- [Visualizing Models, Data, and Training with TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
        "- [A Gentle Introduction to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
      ]
    }
  ]
}