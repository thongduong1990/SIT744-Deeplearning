{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0PWKlbt6_dm"
      },
      "source": [
        "# SIT319/SIT744 Practical 5: Build an Image Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JEtYNgr7JuP"
      },
      "source": [
        "<div class=\"alert alert-info\">\n",
        "We suggest that you run this notebook using Google Colab.\n",
        "</div>\n",
        "\n",
        "## Learning objectives\n",
        "\n",
        "- Construct and train a Convolutional Neural Network\n",
        "\n",
        "## Pre-practical readings\n",
        "\n",
        "- [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 Understanding Loss Functions in PyTorch\n",
        "\n",
        "In this task, we will explore the concept of **loss functions** and how they guide the training process of neural networks. We’ll discuss why loss functions are essential, examine the differences between common regression and classification losses, and implement practical examples in PyTorch.\n",
        "\n",
        "---\n",
        "\n",
        "### Overview of Loss Functions\n",
        "\n",
        "#### What is a Loss Function?\n",
        "A **loss function** (or **cost function**) measures how far off a model’s predictions are from the actual target values. During training, the goal is to **minimize** this loss, guiding the model’s parameters to better fit the data.\n",
        "\n",
        "#### Regression vs. Classification Losses\n",
        "- **Regression Loss Functions**: Used when the output is a continuous value.  \n",
        "  - **Mean Squared Error (MSE)**: Measures the average of the squares of the errors between predictions and targets.  \n",
        "  - **Mean Absolute Error (MAE)**: Measures the average of the absolute differences between predictions and targets.\n",
        "\n",
        "- **Classification Loss Functions**: Used when the output is a discrete class label.  \n",
        "  - **Binary Cross-Entropy (BCE)**: Suitable for binary classification tasks (0 vs. 1).  \n",
        "  - **Cross-Entropy Loss**: Generalization of BCE for multi-class classification. Sometimes referred to as *Softmax Loss* when combined with a softmax layer.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Below, we will illustrate how to implement and compute some of these loss functions using PyTorch’s `torch.nn` module. We’ll set up minimal synthetic examples for both **regression** and **classification** tasks to highlight the key differences.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "47BnsPauKs_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression with MSE Loss\n",
        "\n",
        "Let's create a simple dataset $X$ and a target  $y$ for a regression problem"
      ],
      "metadata": {
        "id": "VFQTYlVkLp-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Synthetic data: y = 2x + 1 with some noise\n",
        "X = torch.randn(10, 1)  # 10 samples, 1 feature\n",
        "y = 2 * X + 1 + 0.2 * torch.randn(10, 1)\n",
        "\n",
        "print(\"Features (X):\\n\", X)\n",
        "print(\"Targets (y):\\n\", y)"
      ],
      "metadata": {
        "id": "LdUkBYvFLwjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll define a single-layer linear model (`nn.Linear`) and compute its output on $X$."
      ],
      "metadata": {
        "id": "LTnAaNi9LZ8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_reg = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "# Forward pass\n",
        "predictions = model_reg(X)\n",
        "print(\"Predictions:\\n\", predictions)"
      ],
      "metadata": {
        "id": "-mkw_z1ZMUa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll instantiate PyTorch's MSELoss and compute the loss between predictions and $y$."
      ],
      "metadata": {
        "id": "IsUebzJdMfCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_mse = nn.MSELoss()\n",
        "loss_mse = criterion_mse(predictions, y)\n",
        "print(\"MSE Loss:\", loss_mse.item())"
      ],
      "metadata": {
        "id": "XwEO3ifZMjXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll notice the scalar loss value. If we train this model (using gradient descent), we can reduce the loss over time to fit our linear data better."
      ],
      "metadata": {
        "id": "PoQniP_tMqUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Classification with BCE Loss\n",
        "\n",
        "We’ll create a small dataset of 0s and 1s to mimic a binary classification scenario."
      ],
      "metadata": {
        "id": "QfG-XLApM-7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 samples, each with 3 \"features\"\n",
        "X_class = torch.randn(10, 3)\n",
        "# Binary labels (0 or 1)\n",
        "y_class = torch.randint(0, 2, (10, 1)).float()\n",
        "\n",
        "print(\"Features (X_class):\\n\", X_class)\n",
        "print(\"Labels (y_class):\\n\", y_class)"
      ],
      "metadata": {
        "id": "QQxt3qRfNaQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll define a single-layer linear model, but we’ll treat it as a logistic regression by passing its output through a sigmoid before computing loss."
      ],
      "metadata": {
        "id": "FxrMoCtZNj6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_class = nn.Linear(in_features=3, out_features=1)\n",
        "\n",
        "# Forward pass\n",
        "logits = model_class(X_class)\n",
        "preds = torch.sigmoid(logits)  # Convert logits to probabilities\n",
        "print(\"Predictions (sigmoid output):\\n\", preds)"
      ],
      "metadata": {
        "id": "f0RMgPFXNk67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provides two main ways to compute BCE loss:\n",
        "\n",
        "1. `nn.BCELoss()` requires manual sigmoid.\n",
        "2. `nn.BCEWithLogitsLoss()` combines sigmoid + BCE in one step.\n",
        "Below, we'll use `BCEWithLogitsLoss()` (common and numerically stable):"
      ],
      "metadata": {
        "id": "T10UaVN8Nrjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_bce = nn.BCEWithLogitsLoss()\n",
        "loss_bce = criterion_bce(logits, y_class)\n",
        "print(\"BCE Loss:\", loss_bce.item())\n"
      ],
      "metadata": {
        "id": "mwGX9y56N0BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: We pass logits directly (no sigmoid needed) to `BCEWithLogitsLoss()`. If you are using `BCELoss()`, you must pass the sigmoid output instead."
      ],
      "metadata": {
        "id": "R6rfTe75N8y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 **Exercise**\n",
        "\n",
        "1. Create a Synthetic Binary Classification Dataset\n",
        "   - Generate 100 data points with 2 input features (e.g., using `torch.randn`).\n",
        "   - Assign binary labels (0 or 1) based on some simple rule (e.g., `label = 1 if x1 + x2 > 0` else `0`).\n",
        "\n",
        "2. Define a Simple Logistic Regression Model\n",
        "   - Use `nn.Linear(in_features=2, out_features=1)` to map your 2D inputs to a single output (logit).\n",
        "   - Remember to apply `torch.sigmoid` if you plan to use `nn.BCELoss`, or pass logits directly if you use `nn.BCEWithLogitsLoss`.\n",
        "\n",
        "3. Train with Different Loss Functions  \n",
        "   1. Using `nn.MSELoss` (**intentionally mismatched for classification**):  \n",
        "      - Set up a training loop for a few epochs (e.g., 50).  \n",
        "      - Observe how the loss decreases and track accuracy on the training data.  \n",
        "   2. Using `nn.BCEWithLogitsLoss` (more appropriate for binary classification):  \n",
        "      - Repeat the training loop.  \n",
        "      - Compare the loss curve and final accuracy with what you obtained using MSE.\n",
        "\n",
        "4. Compare and Discuss  \n",
        "   - Which loss function yields better accuracy and why?  \n",
        "   - What happens if you increase or decrease the number of data points?  \n",
        "   - How does the learning rate affect the training convergence for each loss function?\n",
        "\n"
      ],
      "metadata": {
        "id": "1WkPZ52eOsLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Data Preparation for Image Classification\n",
        "\n",
        "In this task, we will:\n",
        "1. **Load** the Cats vs. Dogs dataset directly from [Hugging Face Datasets](https://huggingface.co/datasets).\n",
        "2. **Split** the data into training, validation, and test sets.\n",
        "3. **Transform** the images (resize, normalize, and augment).\n",
        "4. **Explore** the dataset visually and numerically.\n",
        "\n",
        "Using Hugging Face Datasets allows us to focus on **image classification** without manually downloading or organizing files.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "❗ **If you haven’t installed the `datasets` library yet, run**:"
      ],
      "metadata": {
        "id": "9kQp0bunPQGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "yweaynArX2EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Cats vs. Dogs Dataset\n",
        "\n",
        "Hugging Face hosts a version of the Cats vs. Dogs dataset under `microsoft/cats_vs_dogs`. The code below automatically downloads and caches the dataset."
      ],
      "metadata": {
        "id": "wHjhPWIYYT60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import PIL  # For converting array-based images to PIL before transforms\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Load the entire 'train' split\n",
        "dataset = load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "a-J01lLiYdv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has ~23k images, so we sample a smaller subset for faster experimentation. For example:"
      ],
      "metadata": {
        "id": "I0fLc_4NGxqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and select the first 1,000 images\n",
        "dataset = dataset.shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "ZWKLYIe7G0mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting into Train, Validation, and Test Sets\n",
        "\n",
        "We can use the built-in `train_test_split` method on our `Dataset` object to create train/validation/test subsets."
      ],
      "metadata": {
        "id": "2yf7hpm0G-pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 80% train, 20% temporary (val+test)\n",
        "train_val = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# From the remaining 20%, split equally into val (10%) and test (10%)\n",
        "val_test = train_val[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "train_ds = train_val[\"train\"]  # 80%\n",
        "val_ds   = val_test[\"train\"]   # 10%\n",
        "test_ds  = val_test[\"test\"]    # 10%\n",
        "\n",
        "print(\"Train size:\", len(train_ds))\n",
        "print(\"Val size:\", len(val_ds))\n",
        "print(\"Test size:\", len(test_ds))"
      ],
      "metadata": {
        "id": "DmpPM3vLHKrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Transformations\n",
        "\n",
        "We’ll define a transformation pipeline that:\n",
        "\n",
        "1. Resizes each image to 150×150.\n",
        "Randomly flips images horizontally (data augmentation).\n",
        "2. Normalizes pixel values."
      ],
      "metadata": {
        "id": "r7-5Om6jHS5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.ToImage(),  # converts input image (PIL) to a tv_tensors.Image\n",
        "    T.RandomResizedCrop(\n",
        "        size=(150, 150),\n",
        "        scale=(0.5, 1.0),         # Adjust these values based on how much random zoom you need\n",
        "        ratio=(0.75, 1.33),       # Adjust aspect ratio range if needed\n",
        "        antialias=True\n",
        "    ),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToDtype(torch.float32, scale=True),  # Converts from uint8 [0, 255] to float [0, 1]\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def apply_transform(batch):\n",
        "    # We apply the transform to each one individually:\n",
        "    batch[\"image\"] = [transform(img) for img in batch[\"image\"]]\n",
        "    return batch\n",
        "\n",
        "train_ds.set_transform(apply_transform)\n",
        "val_ds.set_transform(apply_transform)\n",
        "test_ds.set_transform(apply_transform)"
      ],
      "metadata": {
        "id": "L_f-lawHHnQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating PyTorch DataLoaders\n",
        "\n",
        "Now that our subsets are ready, we can create PyTorch `DataLoader`s to handle batching and shuffling:"
      ],
      "metadata": {
        "id": "H9zi1aHhHpEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "fCOymmR-O0mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s inspect a batch of images to ensure they look correct and confirm that augmentation is happening."
      ],
      "metadata": {
        "id": "5KBkcz6vO7nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def denormalize(img_tensor, mean, std):\n",
        "    \"\"\"\n",
        "    Denormalizes a single image tensor using the given mean and std.\n",
        "    img_tensor: (C, H, W)\n",
        "    mean, std: lists of length C\n",
        "    \"\"\"\n",
        "    # clone to avoid modifying tensor in-place\n",
        "    img_tensor = img_tensor.clone().detach()\n",
        "    for c in range(img_tensor.shape[0]):\n",
        "        img_tensor[c] = img_tensor[c] * std[c] + mean[c]\n",
        "    return img_tensor\n",
        "\n",
        "def imshow(img_tensor, mean, std):\n",
        "    # Denormalize\n",
        "    img_tensor = denormalize(img_tensor, mean, std)\n",
        "    # Move channel dimension to the end for plotting\n",
        "    np_img = img_tensor.permute(1, 2, 0).numpy()\n",
        "    # Clip values to valid [0, 1] or [0, 255] range if needed\n",
        "    np_img = np.clip(np_img, 0, 1)\n",
        "    plt.imshow(np_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Fetch one batch\n",
        "batch = next(iter(train_loader))\n",
        "images = batch[\"image\"]\n",
        "labels = batch[\"labels\"]\n",
        "print(\"Batch shape:\", images.shape, labels.shape)\n",
        "\n",
        "# Create a grid of 8 images (2 rows, 4 columns for instance)\n",
        "grid_img = torchvision.utils.make_grid(images[:8], nrow=4)\n",
        "imshow(grid_img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "plt.show()\n",
        "\n",
        "print(\"Labels:\", labels[:8].tolist())  # 0 for cat, 1 for dog (in this dataset)"
      ],
      "metadata": {
        "id": "9-QGvGohO-PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's often helpful to see how many cats vs. dogs are in your dataset. We can quickly iterate over the training set:\n"
      ],
      "metadata": {
        "id": "Av1D5wzSbqAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_labels = []\n",
        "for batch in train_loader:\n",
        "    all_labels.extend(batch[\"labels\"].tolist())\n",
        "\n",
        "counts = Counter(all_labels)\n",
        "print(\"Class 0 (Cats):\", counts[0])\n",
        "print(\"Class 1 (Dogs):\", counts[1])"
      ],
      "metadata": {
        "id": "_4TKIdsTbwxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓ Do you see a large imbalance? If yes, what techniques can you consider to address it?"
      ],
      "metadata": {
        "id": "Rb1ZYovncGIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Building the ConvNet (CNN) Model in PyTorch\n",
        "\n",
        "A CNN typically consists of:\n",
        "- **Convolutional layers** to extract spatial features.\n",
        "- **Activation functions** (e.g., ReLU) to introduce non-linearity.\n",
        "- **Pooling layers** (e.g., MaxPooling) to reduce spatial dimensions and parameters.\n",
        "\n",
        "Below is a simple example of a CNN class with multiple convolutional blocks and a final fully connected (dense) layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "_XIRZyfgcnQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Convolutional Block 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(32)           # Batch Normalization (optional)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)  # Halves the spatial dimensions\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Convolutional Block 3\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        # Use AdaptiveMaxPool2d to get a consistent 9×9 output\n",
        "        self.adaptive_pool = nn.AdaptiveMaxPool2d((9, 9))\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1   = nn.Linear(128*9*9, 512)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout for regularization\n",
        "        self.fc2   = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "\n",
        "        # Adaptive max pooling to 9×9\n",
        "        x = self.adaptive_pool(x)\n",
        "\n",
        "        # Flatten: (batch_size, 128, 9, 9) → (batch_size, 128*9*9)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)  # Dropout\n",
        "        x = self.fc2(x)      # Final layer (logits)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "6HhdANd_wsI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorporating Regularisation\n",
        "\n",
        "- Dropout: Dropout randomly zeros a fraction (`p`) of the neurons during training, reducing overfitting by preventing co-adaptation of features.\n",
        "\n",
        "\n",
        "- Batch Normalization:\n",
        "Applied after each convolutional layer, it normalises the activations. This can accelerate training and improve stability, often allowing higher learning rates.\n",
        "\n",
        "You can experiment with adding/removing dropout or batch normalisation to observe their effects on overfitting."
      ],
      "metadata": {
        "id": "oTHY5mtow_hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Summary\n",
        "\n",
        "After defining the CNN, you can instantiate and print its structure to confirm the number of parameters and layer arrangement."
      ],
      "metadata": {
        "id": "zLnGRPetxnJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN(num_classes=2)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "9b4DUtf6xtF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓What does the output say about each layer?\n",
        "\n",
        "To get the total parameter count, you can do:"
      ],
      "metadata": {
        "id": "v3iTAv10x1IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total Params: {total_params}\")\n",
        "print(f\"Trainable Params: {trainable_params}\")"
      ],
      "metadata": {
        "id": "mPHeKZJ-yTvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Training, Evaluation, and Visualization\n",
        "\n",
        "\n",
        "With the CNN model defined, you’re ready to:\n",
        "\n",
        "- Initialize an optimizer (e.g., Adam or SGD) and loss function (e.g., CrossEntropyLoss for 2-class classification).\n",
        "- Train the model by looping over batches from your data loaders.\n",
        "- Evaluate on validation and test sets."
      ],
      "metadata": {
        "id": "MeEwVvozc108"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Setup\n",
        "\n",
        "1. **Model & Device**: Instantiate your CNN (e.g., `SimpleCNN`) and move it to the appropriate device (CPU or GPU).  \n",
        "2. **Loss Function**: Use `nn.CrossEntropyLoss` for a 2-class (cat vs. dog) classification problem.  \n",
        "3. **Optimizer**: Choose an optimizer like **Adam** or **SGD**.  \n",
        "4. **Data Loaders**: Use the `train_loader` and `val_loader` created in Task 1 for training and validation."
      ],
      "metadata": {
        "id": "NJ8HCz3E0SPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assume SimpleCNN is defined (from Task 3)\n",
        "# and train_loader, val_loader, test_loader are defined (from Task 2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = SimpleCNN(num_classes=2).to(device)  # Move model to GPU if available\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "HZr2Im420eas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training\n",
        "\n",
        "We'll define two helper functions to simplify the main training loop:\n",
        "\n",
        "- `train_one_epoch`: Runs a single epoch over the training set, computing the average loss and accuracy.\n",
        "- `evaluate`: Evaluates on a given data loader (e.g., validation), returning loss and accuracy."
      ],
      "metadata": {
        "id": "MeW8j8gg0oeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        images, labels = batch[\"image\"], batch[\"labels\"]\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects / total_samples\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images, labels = batch[\"image\"], batch[\"labels\"]\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    val_loss = running_loss / total_samples\n",
        "    val_acc = running_corrects / total_samples\n",
        "    return val_loss, val_acc\n",
        "\n",
        "\n",
        "## Main Training Loop\n",
        "num_epochs = 10  # Adjust based on your dataset size and desired training time\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train for one epoch\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "uTxhDmAu05RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation and Visualization\n",
        "\n",
        "We can plot training curves to visualize how loss and accuracy change over epochs:"
      ],
      "metadata": {
        "id": "9YDL5jOl2kB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(1, num_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Val Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(epochs_range, val_accuracies, label='Val Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0TnzJK1Q2-D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓ Do you observe overfitting or underfitting?\n",
        "\n",
        "\n",
        "After choosing your best model (e.g., the one at the last epoch or with the best validation accuracy), evaluate on unseen test data:"
      ],
      "metadata": {
        "id": "wT-YgFdV3DFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "AHeHiAZx3RDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to check how the model performs on individual test images. You can display a few images and compare predictions to ground truth labels:"
      ],
      "metadata": {
        "id": "GBcnTC963Y0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "model.eval()\n",
        "data_iter = iter(test_loader)\n",
        "batch = next(data_iter)\n",
        "images, labels = batch[\"image\"], batch[\"labels\"]\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "# Get predictions\n",
        "outputs = model(images)\n",
        "_, preds = torch.max(outputs, 1)\n",
        "\n",
        "# Move tensors back to CPU for plotting\n",
        "images = images.cpu()\n",
        "labels = labels.cpu()\n",
        "preds = preds.cpu()\n",
        "\n",
        "# Display some images with predicted vs. actual labels\n",
        "class_names = ['cat', 'dog']  # Adjust if your dataset is reversed\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(8):\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    img = denormalize(images[i], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title(f\"Pred: {class_names[preds[i]]} | True: {class_names[labels[i]]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iCggOrEZ3dJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving model\n",
        "\n",
        "You can save your trained model's parameters for future use or inference:"
      ],
      "metadata": {
        "id": "nZGzIOYV3iw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'cats_vs_dogs_cnn.pth')\n",
        "print(\"Model saved as cats_vs_dogs_cnn.pth\")"
      ],
      "metadata": {
        "id": "7OFMdk473qra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later you can load it back."
      ],
      "metadata": {
        "id": "vf5fOAi-3sQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_loaded = SimpleCNN(num_classes=2)\n",
        "model_loaded.load_state_dict(torch.load('cats_vs_dogs_cnn.pth'))\n",
        "model_loaded.to(device)\n",
        "model_loaded.eval()"
      ],
      "metadata": {
        "id": "H_Eq8gJP3ySB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have a complete end-to-end pipeline for cats vs. dogs classification using PyTorch: from dataset preparation (Task 2) and model building (Task 3) to training and evaluation (Task 4)."
      ],
      "metadata": {
        "id": "rMh0CzUA3_3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Exercises\n",
        "\n",
        "### Exercise 1: Custom Data Splitting\n",
        "\n",
        "   - Instead of using automatic splitting (like `train_test_split`), manually create `train/`, `val/`, and `test/` folders.  \n",
        "   - Ensure each split has a balanced distribution of cats and dogs (e.g., 80% train, 10% val, 10% test).\n",
        "   - Print how many cat vs. dog images are in each split. Check for imbalance.\n",
        "\n",
        "\n",
        "### Exercise 2: More Data Augmentation\n",
        "\n",
        "   - Extend the `train_transform` pipeline with `RandomRotation`, `ColorJitter`, or `RandomResizedCrop`.  \n",
        "   - Compare training performance (loss/accuracy) with and without these additional augmentations.\n",
        "   - Temporarily remove *all* augmentations. Observe if the model overfits more quickly (e.g., higher training accuracy, lower validation accuracy).\n",
        "   - Display 5 augmented images (with flips, rotations, color jitter) to see how they differ from the originals.\n",
        "\n"
      ],
      "metadata": {
        "id": "7obtnRI04wDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
      ],
      "metadata": {
        "id": "bZjb3nFK6Qgq"
      }
    }
  ]
}